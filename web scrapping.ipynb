{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae0252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get websote title\n",
    "'''\n",
    "=======>step 1 :Get the html using request(to get the content of html as string)\n",
    "=======>step 2 :Parse the html using beautiful soap\n",
    "=======>step 3 :Now Traverse that Tree\n",
    "'''\n",
    "\n",
    "'''\n",
    "Beautiful Soup is a Python \n",
    "package for parsing HTML and XML documents \n",
    "(including having malformed markup, i.e. non-closed tags, \n",
    "so named after tag soup). It creates\n",
    "a parse tree for parsed pages that can be used to extract data from HTML3 which is useful for web scraping.\n",
    "since it does not have request fucntion so we use request library seprately\n",
    "'''\n",
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "#step 1:    \n",
    "\n",
    "res= requests.get('#website URL to be scraped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d262f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<Response 200 means request request send to above URL is accepted>\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8065917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#res is request class object\n",
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285ce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2: \n",
    "soup=bs4.BeautifulSoup(res.text,'html.parser')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d54550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3:\n",
    "title=soup.title\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42077128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to print first paragraphs\n",
    "print(soup.find('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d325bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors=soup.find_all('a') # a:anchors here is an array to get all anchor link we do\n",
    "\n",
    "# to print all links:\n",
    "\n",
    "all_links=set() # To remove duplicate element\n",
    "\n",
    "print(anchors[1].get('href'))\n",
    "\n",
    "website=\"https://abc.com/\" #Your wesite link here\n",
    "for link in anchors:\n",
    "    linktext=website+ str(link.get('href'))\n",
    "    all_links.add(linktext)\n",
    "for i in all_links:\n",
    "    print(i)\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef937b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "con=soup.find(id='main')\n",
    "for element in con.children:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e99d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To print all the strings in websitea website we use con.stripped_strings:\n",
    "pract=set()\n",
    "con=soup.find(id='main')\n",
    "for element in con.stripped_strings:\n",
    "      pract.add(element)\n",
    "for i in pract:\n",
    "    print(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f='China selects pilot zones, application areas for blockchain project'\n",
    "pract=set()\n",
    "con1=soup.find(id='main')\n",
    "anch=soup.find_all('h2')\n",
    "for i in anch:\n",
    "    print(str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8373830",
   "metadata": {},
   "outputs": [],
   "source": [
    "div = soup.findAll('div')\n",
    "\n",
    "content = str(div)\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3b6e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "anchors=soup.find_all('a')\n",
    "all_links=[]\n",
    "for i in anchors:\n",
    "    pract=website+str(i.get('href'))\n",
    "    x=pract\n",
    "    y=x.split('/https')[0]\n",
    "    all_links.append(y)\n",
    "all_links[0]\n",
    "my_series=pd.Series(data=all_links)\n",
    "my_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e8dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "links=pd.DataFrame({\"Links\":my_series})\n",
    "links.shape #(218,1)==> (rows,columns)\n",
    "links.duplicated().sum() #==> 100 duplicates in links dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3c00a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8241d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "links.shape  #==> now (99,1) ==>(rows,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d9f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47701f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "req=requests.get('https://dunyanews.tv/en/Pakistan/639565-Soldier-martyred,-four-terrorists-killed-in-attacks-on-military-camps-in')\n",
    "sip=bs4.BeautifulSoup(req.text,'html.parser')\n",
    "para=soup.find('p')\n",
    "para\n",
    "tran=[]\n",
    "article=sip.find('article')\n",
    "for i in article:\n",
    "    p=i.text.replace('\\div','').replace('\\n','').replace('\\r','')\n",
    "    tran.append(p)\n",
    " print(tran)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5633e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_URL_response(url):\n",
    "    req=requests.get(url)\n",
    "    soup=bs4.BeautifulSoup(req.text,\"html.parser\")\n",
    "    return(soup)\n",
    "\n",
    "def internal_external_links(soup,domain_name):\n",
    "    hrefs=soup.findAll('a')\n",
    "    links=[]\n",
    "    for i in hrefs:\n",
    "        pure_url=domain_name+str(i.get('href'))\n",
    "        temp=pure_url.split('/https')[0]\n",
    "        links.append(temp)\n",
    "    return links\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "domain_name=\"https://dunyanews.tv/\"\n",
    "soup=main_URL_response(domain_name)\n",
    "links=internal_external_links(soup,domain_name)\n",
    "content=[]\n",
    "for i in links:\n",
    "    x=i.split('/http')[0]\n",
    "    soup=main_URL_response(x)\n",
    "    article=soup.find('section')\n",
    "    #p=article.text.replace('\\div','').replace('\\n','')\n",
    "    content.append(article)\n",
    "link=pd.Series(data=links)\n",
    "content=pd.Series(data=content)\n",
    "scrap=pd.DataFrame({\"Links\":link,\"Content\":content})\n",
    "scrap.drop_duplicates(inplace=True)\n",
    "scrap.to_csv(\"website-data.csv\",index=False)\n",
    "scrap\n",
    "    \n",
    "scrap.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a791fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efddd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b79d39d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852a91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=main_URL_response('https://dunyanews.tv/en/Pakistan/640107-PTI-mortgaged-SBP-meet-shameful-conditions-IMF-alleges-Hamza')\n",
    "get=soup.findAll('p')\n",
    "put=[]\n",
    "put.append(get)\n",
    "print(get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a71da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[]\n",
    "site='https://dunyanews.tv/en/Pakistan/640107-PTI-mortgaged-SBP-meet-shameful-conditions-IMF-alleges-Hamza'\n",
    "scrap(site)\n",
    "def scrap(site):\n",
    "    res= requests.get(site)\n",
    "    s=BeautifulSoup(res.content,\"html.parser\")\n",
    "    for i in s.findAll('href'):\n",
    "        lst.append(i)\n",
    "       # href=i.attrs['href']\n",
    "       # if href.startswith(\"/\"):\n",
    "        #    site=site+href\n",
    "         #   if site not in lst:\n",
    "          #      lst.append(site)\n",
    "           #     print(site)\n",
    "            #    scrap(site)\n",
    "#scrap(site)\n",
    "print(lst)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97cbe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "res= requests.get('https://dunyanews.tv/')\n",
    "bsoup=BeautifulSoup(res.text,'html.parser')\n",
    "link_lst=bsoup.find_all('a')\n",
    "lst=[]\n",
    "for link in link_lst:\n",
    "    if 'href' in link.attrs:\n",
    "        x=str(\"https://dunyanews.tv/\"+ str(link.attrs['href']))\n",
    "        y=x.split('/https')[0]\n",
    "        lst.append(y)\n",
    "linked=pd.Series(data=links)\n",
    "sem=pd.DataFrame({\"Links\":linked})\n",
    "sem.drop_duplicates(inplace=True)\n",
    "for i in sem.iterrows():\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af41fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sem:\n",
    "    linkess=sem.loc[rand]\n",
    "    res= requests.get(linkess)\n",
    "    bpsoup=BeautifulSoup(res.content,'html.parser')\n",
    "    para=bpsoup.find('p')\n",
    "    lst2.append(para)\n",
    "    rand=rand+1\n",
    "\n",
    "print(last2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e6e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=[]\n",
    "def main_URL_response(url):\n",
    "    req=requests.get(url)\n",
    "    soup=bs4.BeautifulSoup(req.text,\"html.parser\")\n",
    "    for data in soup.find('p'):\n",
    "        temp.append(data.get_text())\n",
    "\n",
    "        \n",
    "def internal_external_links(soup,domain_name):\n",
    "    hrefs=soup.findAll('a')\n",
    "    links=[]\n",
    "    for i in hrefs:\n",
    "        pure_url=domain_name+str(i.get('href'))\n",
    "        temp=pure_url.split('/https')[0]\n",
    "        links.append(temp)\n",
    "    return links\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "domain_name=\"https://dunyanews.tv/\"\n",
    "soup=main_URL_response(domain_name)\n",
    "links=internal_external_links(soup,domain_name)\n",
    "content=[]\n",
    "for i in links:\n",
    "    x=i.split('/http')[0]\n",
    "    soup=main_URL_response(x)\n",
    "    article=soup.find('p').get_text()\n",
    "    content.add(article)\n",
    "    #p=article.text.replace('\\div','').replace('\\n','')\n",
    "link=pd.Series(data=links)\n",
    "content=pd.Series(data=temp)\n",
    "scrap=pd.DataFrame({\"Links\":link,\"Content\":content})\n",
    "scrap.drop_duplicates(inplace=True)\n",
    "scrap.to_csv(\"website-data.csv\",index=False)\n",
    "scrap\n",
    "scrap.dropna(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea28e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap.dropna(inplace=True)\n",
    "scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3dedf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wraping all the above code into fucntion and gatering them at one place as under\n",
    "\n",
    "def main_URL_response(url):\n",
    "    req=requests.get(url)\n",
    "    soup=bs4.BeautifulSoup(req.text,\"html.parser\")\n",
    "    return(soup)\n",
    "\n",
    "def internal_external_links(soup,domain_name):\n",
    "    hrefs=soup.findAll('a')\n",
    "    links=[]\n",
    "    for i in hrefs:\n",
    "        pure_url=domain_name+str(i.get('href'))\n",
    "        temp=pure_url.split('/https')[0]\n",
    "        links.append(temp)\n",
    "    return links\n",
    "    \n",
    "    \n",
    "\n",
    "links=[]\n",
    "content=[]\n",
    "    \n",
    "domain_name=\"https://dunyanews.tv/\"\n",
    "soup=main_URL_response(domain_name)\n",
    "links=internal_external_links(soup,domain_name)\n",
    "for i in links:\n",
    "    x=i.split('/http')[0]\n",
    "    links.append(x)\n",
    "    soup=main_URL_response(x)\n",
    "    article=soup.find_all('p')\n",
    "    #p=article.text.replace('\\div','').replace('\\n','')\n",
    "    content.append(article)\n",
    "link=pd.Series(data=links)\n",
    "content=pd.Series(data=content)\n",
    "scrap=pd.DataFrame({\"Links\":link,\"Content\":content})\n",
    "scrap.drop_duplicates(inplace=True)\n",
    "scrap.to_csv(\"website-data.csv\",index=False)\n",
    "scrap\n",
    "    \n",
    "scrap.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87590826",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c436c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
